{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KugzR3ZrbKWO"
      },
      "source": [
        "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
        "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaS4gR-TbKWP"
      },
      "outputs": [],
      "source": [
        "#%pip install pandas\n",
        "#%pip install matplotlib\n",
        "# add commented pip installation lines for packages used as shown above for ease of testing\n",
        "# the line should be of the format %pip install PACKAGE_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV2-uiFpbKWQ"
      },
      "source": [
        "## **DO NOT CHANGE** the filepath variable\n",
        "##### Instead, create a folder named 'data' in your current working directory and\n",
        "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCw9cxyybKWQ"
      },
      "outputs": [],
      "source": [
        "# Can have as many cells as you want for code\n",
        "import pandas as pd\n",
        "filepath = \"./data/catB_train.parquet\"\n",
        "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPBgHU_7bKWQ"
      },
      "source": [
        "### **ALL** Code for machine learning and dataset analysis should be entered below.\n",
        "##### Ensure that your code is clear and readable.\n",
        "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspiration\n",
        "Upon initial analysis of the data, we have found that most relevant columns are binary in nature. Thus, initially, we opted for a decision tree classifier for its interpretability. However, we later found the dataset to be rather imbalanced with only 700 positive cases out of 18,992 rows, leading us to switch to a Random Forest model. We were inspired to use this model as it's ability to build multiple decision trees and aggregate predictions not only reduces the risk of overfitting but also effectively handles imbalanced classes. In the end, this model has helped to produce a more accurate solution to predict customer satisfaction in the insurance acquisition process.\n",
        "\n",
        "## How we built it \n",
        "The product was created under the notion of trying out different types of classification algorithms. After testing out several, we found that Decision Trees was quite suitable for our choice of using TRUE FALSE columns. As we tested, we realised the accuracy was not up to our standard and we started to look for variants of this algorithm. It was here that we found the Random Forest Classification Method.\n",
        "\n",
        "As such, after filtering out the columns that we wanted and changed 'stat_flag' into three separate columns for TRUE FALSE use, we can dropped all of the NAs and proceeded to train the model with class weights that was suitable for the problem. With that, we tested it out using a randomly selected test dataframe used a confusion matrix to check our accuracy.\n",
        "\n",
        "In this state, the test results for the dataset was \n",
        "True Negative (TN): 3058\n",
        "False Positive (FP): 186\n",
        "False Negative (FN): 120\n",
        "True Positive (TP): 32\n",
        "To calculate accuracy using the formula \n",
        "Accuracy = True Positives / (False Positives + True Positives)\n",
        "\n",
        "Therefore, the Accuracy was 32/218 = 0.147 (rounded).\n",
        "\n",
        "## Challenges we ran into \n",
        "Choosing the suitable machine learning model was one of the main challenges we faced as we realised some models we considered at the beginning do not work well with the dataset. Trial and error to find the best model took quite a significant amount of time which was not very desirable especially with the short duration given. There were very few true positive cases in the dataset which made it more challenging to split training data and test data to train our model.\n",
        "\n",
        "## What we learned \n",
        "Throughout the past 3 days, my team and I was able to apply what we have learnt in the classroom, experimenting with different classifiers and figuring the strengths and limitations of each classifier. We have tried many classifiers, such as KNN, SVM, Decision Tree and Random Forest, and found out through trial and error that, for example, Decision Tree in this context was not the most suitable due to the skewed nature of the dataset. We eventually settled on Random Forest, as we learned through comparing the accuracy that it was the most suitable model for this dataset.\n",
        "\n",
        "We also learned how the different factors, such as status and purchase history were able to drastically affect the customer's propensity, which was learned through using different variables and seeing how the training data compared with the test data. This allowed us to understand how each factor in the Insurance Industry can impact the consumer's decision, and taught us the importance of data analytics and machine learning in this industry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "r7UZHjKceFQz",
        "outputId": "7bca5e3f-535f-4090-e232-fef18952de15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[3058  186]\n",
            " [ 120   32]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "df = pd.read_parquet(filepath)\n",
        "\n",
        "# Convert target col to 0 or 1 \n",
        "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0) \n",
        "#Select unique client number variables and target column \n",
        "df = df[[\"flg_substandard\",\"flg_is_borderline_standard\",\"flg_is_revised_term\",\"flg_is_rental_flat\",\"flg_has_health_claim\",\"flg_has_life_claim\",\"flg_gi_claim\",\"flg_is_proposal\",\"flg_with_preauthorisation\",\"flg_is_returned_mail\",\"f_purchase_lh\"]] \n",
        "#Remove rows with NA \n",
        "df = df.dropna()\n",
        "\n",
        "# Filter out the columns you want\n",
        "selected_columns = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', \n",
        "                    'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
        "                    'flg_is_returned_mail', 'f_purchase_lh']  # Replace with your actual column names\n",
        "df_selected = df[selected_columns]\n",
        "\n",
        "# Assuming your prediction target variable is called 'f_purchase_lh'\n",
        "X = df_selected.drop('f_purchase_lh', axis=1)\n",
        "y = df_selected['f_purchase_lh']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "model = RandomForestClassifier(class_weight={0: 1, 1: 17})  # You can set class weights if needed\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VdbPbk8bKWQ"
      },
      "source": [
        "## The cell below is **NOT** to be removed\n",
        "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list).\n",
        "##### It is recommended to test the function out prior to submission\n",
        "-------------------------------------------------------------------------------------------------------------------------------\n",
        "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
        "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zc6KOxOybKWQ"
      },
      "outputs": [],
      "source": [
        "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
        "    df = hidden_data\n",
        "    #Select unique client number variables and target column \n",
        "    df = df[[\"flg_substandard\",\"flg_is_borderline_standard\",\"flg_is_revised_term\",\"flg_is_rental_flat\",\"flg_has_health_claim\",\"flg_has_life_claim\",\"flg_gi_claim\",\"flg_is_proposal\",\"flg_with_preauthorisation\",\"flg_is_returned_mail\"]] \n",
        "    #Remove rows with NA \n",
        "    df = df.dropna()\n",
        "\n",
        "    # Filter out the columns you want\n",
        "    selected_columns = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', \n",
        "                    'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
        "                    'flg_is_returned_mail']  # Replace with your actual column names\n",
        "    df_selected = df[selected_columns]\n",
        "\n",
        "    result = model.predict(df_selected)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLHF3YM2bKWR"
      },
      "source": [
        "##### Cell to check testing_hidden_data function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCVDJtD9bKWR"
      },
      "outputs": [],
      "source": [
        "# This cell should output a list of predictions.\n",
        "test_df = pd.read_parquet(filepath)\n",
        "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
        "print(testing_hidden_data(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLkmKzyMbKWR"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
